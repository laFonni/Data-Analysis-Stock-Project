# -*- coding: utf-8 -*-
"""exam for students.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1buvXJ9FDDHPFA7cuGdUkAYGYQtO0T4aA
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("spscientist/students-performance-in-exams")

print("Path to dataset files:", path)

import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split

import shutil
import os

# Docelowy folder - np. lokalny w Colab
destination_folder = '/content/students/1'

# Utwórz folder jeśli nie istnieje
os.makedirs(destination_folder, exist_ok=True)



print(f'Pliki przeniesione do: {destination_folder}')



df = pd.read_csv(destination_folder + "/1/StudentsPerformance.csv")

scaler = MinMaxScaler()

for col in ['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course']:
    df[col] = LabelEncoder().fit_transform(df[col])
    df[col] = scaler.fit_transform(df[[col]])

df['pass_math'] = np.where(df['math score'] >= 40, 0, 1)
df['pass_reading'] = np.where(df['reading score'] >= 40, 1, 0)
df['pass_writing'] = np.where(df['writing score'] >= 40, 1, 0)

df = df.drop(columns=['math score', 'reading score', 'writing score'])
df.head()

X = df.drop(columns=['pass_math', 'pass_reading', 'pass_writing']).values
y = df['pass_math'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train = torch.tensor(X_train, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)
y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

print(torch.bincount(y_train.squeeze(1).int()))
print(torch.bincount(y_test.squeeze(1).int()))

class LogisticRegression(nn.Module):
    def __init__(self, input_dim):
        super(LogisticRegression, self).__init__()
        self.linear = nn.Linear(input_dim, 1)

        self.fc1 = nn.Linear(input_dim, 32)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(32, 1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu1(x)
        x = self.fc2(x)
        return x

model = LogisticRegression(X_train.shape[1])

neg_count = (y_train == 0).sum().item()  # zdał
pos_count = (y_train == 1).sum().item()  # nie zdał

pos_weight_value = neg_count / pos_count  # 771 / 29 ≈ 26.6
pos_weight = torch.tensor([pos_weight_value])


criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

epochs = 1000
for epoch in range(epochs):
    model.train()

    outputs = model(X_train)

    loss = criterion(outputs, y_train)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch+1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

final_threshold = 0.68

model.eval()
with torch.no_grad():
    y_pred = model(X_test)
    y_pred = (y_pred > final_threshold).float()
    accuracy = (y_pred == y_test).float().mean()
    print(f'Test Accuracy: {accuracy.item():.4f}')

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve, classification_report
import matplotlib.pyplot as plt

# Klasyczne metryki
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"Precision: {precision_score(y_test, y_pred):.4f}")
print(f"Recall: {recall_score(y_test, y_pred):.4f}")
print(f"F1-score: {f1_score(y_test, y_pred):.4f}")
print(f"AUC: {roc_auc_score(y_test, y_pred):.4f}")

# Macierz pomyłek
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

# Krzywa ROC
fpr, tpr, thresholds = roc_curve(y_test, y_pred)
plt.plot(fpr, tpr)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate (Recall)")
plt.title("ROC Curve")
plt.show()

import numpy as np
from sklearn.metrics import precision_recall_fscore_support

# predicted probabilities (y_proba)
with torch.no_grad():
    logits = model(X_test)
    probabilities = torch.sigmoid(logits)  # predicted probabilities → y_proba


precisions = []
recalls = []

thresholds = np.arange(0.0, 1.0, 0.01)

for threshold in thresholds:
    y_pred = (probabilities >= threshold).float()  # to jest ten ważny moment!

    # Jeśli odwracałeś klasy (1 = nie zdał), to nic nie zmieniasz, jak nie, to: y_pred = 1 - y_pred
    y_pred_final = y_pred

    precision, recall, f1, _ = precision_recall_fscore_support(
        y_test.cpu(), y_pred_final.cpu(), average='binary', pos_label=1
    )

    precisions.append(precision)
    recalls.append(recall)

    print(f"Threshold: {threshold:.2f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}")

plt.plot(recalls, precisions, marker='o')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.grid(True)
plt.show()

best_f1 = 0
best_threshold = 0
best_precision = 0
best_recall = 0

thresholds = np.arange(0.0, 1.0, 0.01)

for threshold in thresholds:
    y_pred = (probabilities >= threshold).float()
    y_pred_final = y_pred

    precision, recall, f1, _ = precision_recall_fscore_support(
        y_test.cpu(), y_pred_final.cpu(), average='binary', pos_label=1
    )

    if f1 > best_f1:
        best_f1 = f1
        best_threshold = threshold
        best_precision = precision
        best_recall = recall

print(f"Best Threshold: {best_threshold:.2f}")
print(f"Precision: {best_precision:.4f} | Recall: {best_recall:.4f} | F1: {best_f1:.4f}")

import xgboost as xgb

dtrain = xgb.DMatrix(X_train.cpu(), label=y_train.cpu())
dtest = xgb.DMatrix(X_test.cpu(), label=y_test.cpu())

params = {
    'objective': 'binary:logistic',
    'eval_metric': 'auc',
    'scale_pos_weight': (y_train == 0).sum().item() / (y_train == 1).sum().item()
}

bst = xgb.train(params, dtrain, num_boost_round=100)

y_pred_proba = bst.predict(dtest)
y_pred = (y_pred_proba >= 0.70).astype(int)

print(classification_report(y_test.cpu(), y_pred))
print(confusion_matrix(y_test.cpu(), y_pred))

# !pip uninstall numpy xgboost catboost
# !pip install numpy==1.23.5
# !pip install xgboost
# !pip install catboost

!pip install numpy==1.23.5 --force-reinstall

!pip install catboost --force-reinstall

from catboost import CatBoostClassifier
from sklearn.metrics import classification_report, confusion_matrix

# CatBoost potrzebuje numpy, więc konwertujemy jeśli masz tensory
X_train_np = X_train.cpu().numpy()
X_test_np = X_test.cpu().numpy()
y_train_np = y_train.cpu().numpy()
y_test_np = y_test.cpu().numpy()

# Policzenie wag klas
neg_count = (y_train_np == 0).sum()
pos_count = (y_train_np == 1).sum()
class_weights = [1.0, neg_count / pos_count]  # [weight_for_0, weight_for_1]

print(f"Class weights: {class_weights}")

# Trening CatBoost
model = CatBoostClassifier(
    iterations=1000,
    learning_rate=0.05,
    depth=6,
    eval_metric='AUC',
    random_seed=42,
    verbose=100,
    class_weights=class_weights
)

model.fit(X_train_np, y_train_np, eval_set=(X_test_np, y_test_np), early_stopping_rounds=50)

# Predykcja
y_pred_proba = model.predict_proba(X_test_np)[:, 1]

# Próg decyzyjny
threshold = 0.68
y_pred = (y_pred_proba >= threshold).astype(int)

# Raporty
print(classification_report(y_test_np, y_pred, target_names=['Zdał', 'Nie zdał']))
print("Confusion Matrix:\n", confusion_matrix(y_test_np, y_pred))